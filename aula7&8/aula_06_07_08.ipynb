{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este material é um mix de material próprio, e material adaptado do livro texto da disciplina e do repositório do autor do livro (https://github.com/ageron/handson-ml)\n",
    "\n",
    "Material de referência: livro do Géron, capítulo 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos lineares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Até o momento vimos o processo de construção de uma solução de machine learning:\n",
    "\n",
    "- Aquisição e preparo dos dados\n",
    "- Definição da métrica de desempenho\n",
    "- Separação em conjuntos de treinamento e teste\n",
    "- Investigação dos dados\n",
    "- Escolha de modelo:\n",
    "    - Avaliação de vários modelos usando validação cruzada\n",
    "    - Comparação de métricas de desempenho\n",
    "    - Ajuste de hiperparâmetros\n",
    "- Treinamento final\n",
    "- Teste final\n",
    "- Etapas de producão\n",
    "    - Relatório final\n",
    "    - Operacionalização do modelo em produção\n",
    "\n",
    "Porém neste processo todo nossos modelos eram \"caixas pretas\", não conhecemos nada sobre como os modelos funcionam!\n",
    "\n",
    "Nesta aula vamos começar a investigar os modelos lineares: o que são, e como treina-los. Vamos aprender sobre regressão linear, algoritmos de treinamento, regressão polinomial, overfitting versus underfitting. Na aula seguinte vamos discutir o *tradeoff bias/variance*, estratégias de regularização, regressão logística e softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade:**\n",
    "\n",
    "Sem olhar no material de suporte, liste no papel os passos fundamentais de um projeto de machine learning. Vocë tem que conhecer isso de memória!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos inicializar algumas bibliotecas padrão de nosso trabalho:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "RAND_SEED = 42\n",
    "\n",
    "np.random.seed(RAND_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Revisão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Em todo problema de machine learning uma amostra de entrada é um vetor \n",
    "\n",
    "$$\\mathbf{x} = (x_1, x_2, \\cdots, x_n) \\in \\mathbb{R}^{n}$$\n",
    "\n",
    "de $n$ valores (características ou *features*) reais. (Valores categóricos tem que ser convertidos em valores reais usando codificação *one-hot*.)\n",
    "\n",
    "Em notação matricial, os vetores são matrizes-coluna:\n",
    "\n",
    "$$\n",
    "\\mathbf{x} = \\left[\n",
    "\\begin{matrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "\\vdots \\\\\n",
    "x_n\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "\\in \\mathcal{M}_{n \\times 1}(\\mathbb{R})\n",
    "$$\n",
    "\n",
    "Vamos trabalhar com as duas notações (vetores e matrizes-coluna) de modo intercambiável, e que ficará evidente no contexto de uso.\n",
    "\n",
    "Uma amostra de saída é um valor que se supõe depender da entrada. Se o valor de saída é contínuo temos um problema de **regressão**, e se o valor é categórico (trata-se de uma classe ou categoria) temos um problema de **classificação**.\n",
    "\n",
    "Um modelo de machine learning é uma coleção de funções paramétricas, que permitem estimar o valor de saída para uma dada amostra de entrada. Os **parâmetros** $\\mathbf{\\theta}$ representam os \"ajustes\" que permitem escolher a melhor função para nossa tarefa. Ou seja:\n",
    "\n",
    "$$y_{\\text{estimado}} = \\hat{y} = h_{\\mathbf{\\theta}}\\left(\\mathbf{x}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Erro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O objetivo principal em um projeto de machine learning é encontrar um modelo que produza boas estimativas, ou seja, estimativas que se aproximem dos valores reais. *Treinamento* é ajustar os parâmetros $\\mathbf{\\theta}$ para conseguir **em geral** melhores estimativas. (Ou seja, não é garantido que acerta em todos os casos, mas tenta estar *quase sempre próximo*.)\n",
    "\n",
    "Uma estimativa é boa se o **erro** entre a estimativa e o valor real é baixo. A definição do erro depende do problema em questão. Se for uma regressão, então uma medida razoável de erro para uma dada amostra é a diferença entre o valor real e o valor estimado:\n",
    "\n",
    "$$\\text{erro}\\left(\\mathbf{x}, y_{real}, \\mathbf{\\theta} \\right) = y_{\\text{estimado}} - y_{\\text{real}} = h_{\\mathbf{\\theta}}\\left(\\mathbf{x}\\right) - y_{\\text{real}}$$\n",
    "\n",
    "Se desejamos uma métrica positiva de erro, podemos adotar:\n",
    "\n",
    "- o valor absoluto do erro: $\\left| \\,\\text{erro}\\left(\\mathbf{x}, y_{real}, \\mathbf{\\theta} \\right) \\,\\right|$, ou \n",
    "- o valor quadrático do erro: $\\text{erro}\\left(\\mathbf{x}, y_{real}, \\mathbf{\\theta} \\right)^2$\n",
    "\n",
    "Se for um problema de classificação, então o erro pode ser simplesmente o resultado da comparação entre a classe real e a classe estimada. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Erro sobre um conjunto de amostras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Será que um modelo é bom? Uma forma de responder esta questão é avaliar o erro médio do classificador sobre um conjunto de exemplos. \n",
    "\n",
    "Para ilustrar essa ideia, vamos considerar um problema de regressão. Seja $\\left(\\mathbf{x}^{(1)}, \\mathbf{x}^{(2)}, \\cdots, \\mathbf{x}^{(m)}\\right)$ uma lista de $m$ amostras, e $\\left(y_{\\text{real}}^{(1)}, y_{\\text{real}}^{(2)}, \\cdots, y_{\\text{real}}^{(m)}\\right)$ os valores dependentes correspondentes.\n",
    "\n",
    "(Lembrando: $\\mathbf{x}^{(i)} \\in \\mathcal{M}_{n \\times 1}(\\mathbb{R})$ são matrizes-coluna em notação matricial, e $y_{\\text{real}}^{(i)} \\in \\mathbb{R}$ são valores reais.)\n",
    "\n",
    "Vamos representar essa informação na forma de uma matriz $\\mathbf{X} \\in \\mathcal{M}_{m \\times n}(\\mathbb{R})$ para as amostras e um vetor $\\mathbf{y}_{\\text{real}}$ para os valores dependentes, ou seja:\n",
    "\n",
    "$$\n",
    "\\mathbf{X} = \\left[\n",
    "\\begin{matrix} \n",
    "\\left(\\mathbf{x}^{(1)}\\right)^{T} \\\\\n",
    "\\left(\\mathbf{x}^{(2)}\\right)^{T} \\\\\n",
    "\\vdots \\\\\n",
    "\\left(\\mathbf{x}^{(m)}\\right)^{T} \\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "\\  \\text{,} \\quad\n",
    "\\mathbf{y}_{\\text{real}} = \\left[\n",
    "\\begin{matrix}\n",
    "y_{\\text{real}}^{(1)} \\\\\n",
    "y_{\\text{real}}^{(1)} \\\\\n",
    "\\vdots \\\\\n",
    "y_{\\text{real}}^{(1)} \\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "O erro quadrático médio é (surpresa!) a média do erro quadrático sobre o conjunto de amostras:\n",
    "\n",
    "$$\n",
    "\\varepsilon^2\\left(\\mathbf{X}, \\mathbf{y}_{\\text{real}}, \\mathbf{\\theta}\\right) = \n",
    "\\frac{1}{m} \\sum_{i=1}^{m} \\left(h_{\\mathbf{\\theta}}\\left(\\mathbf{x}^{(i)}\\right) - y_{\\text{real}}^{(i)}\\right)^2\n",
    "$$\n",
    "\n",
    "É comum usarmos a raiz quadrada de $\\varepsilon^2\\left(\\mathbf{X}, \\mathbf{y}_{\\text{real}}, \\mathbf{\\theta}\\right)$, para que o erro esteja descrito nas mesmas unidades de medida da variável dependente. Obtemos então a medida de erro médio conhecida como RMSE (*Root Mean Squared Error*), muito usada em problemas de regressão.\n",
    "\n",
    "$$\n",
    "\\text{RMSE}\\left(\\mathbf{X}, \\mathbf{y}_{\\text{real}}, \\mathbf{\\theta}\\right) \n",
    "= \n",
    "\\sqrt{\\varepsilon^2\\left(\\mathbf{X}, \\mathbf{y}_{\\text{real}}, \\mathbf{\\theta}\\right)}\n",
    "=\n",
    "\\sqrt{\n",
    "    \\frac{1}{m} \n",
    "    \\sum_{i=1}^{m} \n",
    "        \\left(h_{\\mathbf{\\theta}}\\left(\\mathbf{x}^{(i)}\\right) - y_{\\text{real}}^{(i)}\\right)^2\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treinar um modelo é achar os parâmetros do modelo que minimizam o erro médio deste em um conjunto de amostras de treinamento. Por exemplo, para o problema de regressão e usando RMSE como métrica de desempenho, temos matematicamente:\n",
    "\n",
    "$$\n",
    "\\mathbf{\\theta}_{\\text{opt}} = \\text{arg min}_{\\mathbf{\\theta}}\\left\\{\\text{RMSE}\\left(\\mathbf{X}_{\\text{train}}, \\mathbf{y}_{\\text{train}}, \\mathbf{\\theta}\\right)\\right\\}\n",
    "$$\n",
    "\n",
    "Para procurar $\\mathbf{\\theta}_{\\text{opt}}$ temos que aplicar um **algoritmo de treinamento** ao nosso modelo e nossas amostras de treinamento. Para cada modelo e métrica de desempenho existe um algoritmo de treinamento. Alguns algoritmos de treinamento são versáteis, e são aplicáveis a vários tipos de modelo - por exemplo, o *método do máximo declive* (**gradient descent**).\n",
    "\n",
    "Em termos de programação temos o seguinte: criamos um *objeto* que representa o nosso modelo, e usamos um método (e.g. *fit*) para iniciar o treinamento.\n",
    "\n",
    "$$\\text{modelo} = \\text{Modelo}()$$\n",
    "\n",
    "$$\\mathbf{\\theta}_{\\text{opt}} = \\text{modelo.fit}(\\mathbf{X}_{\\text{train}}, \\mathbf{y}_{\\text{train}})$$\n",
    "\n",
    "Existem parâmetros que não são ajustáveis neste processo, e seus valores devem ser passados para o algoritmo de treinamento. Por exemplo: número máximo de iterações do método de treinamento, valor do parâmetro de regularização, etc. Estes parâmetros extras sao chamados de **hiperparâmetros**. Portanto, é mais correto escrever a expressão acima como:\n",
    "\n",
    "$$\\text{modelo} = \\text{Modelo}(\\mathbf{\\theta}_{\\text{hiper}})$$\n",
    "\n",
    "$$\\mathbf{\\theta}_{\\text{opt}} = \\text{modelo.fit}(\\mathbf{X}_{\\text{train}}, \\mathbf{y}_{\\text{train}})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usando o modelo treinado para fazer predições"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, quando temos o modelo treinado (ou seja, encontramos os parâmetros ótimos $\\mathbf{\\theta}$), podemos usá-lo para fazer predições: seja $\\mathbf{x}_{\\text{novo}}$ uma nova amostra, então a predição $\\hat{y}$ do modelo é calculada como:\n",
    "\n",
    "$$\\hat{y} = h_{\\mathbf{\\theta}_{\\text{opt}}; \\mathbf{\\theta}_{\\text{hiper}}}\\left(\\mathbf{x}_{\\text{novo}}\\right)$$\n",
    "\n",
    "Ou, em pseudo-código:\n",
    "\n",
    "$$\\text{modelo} = \\text{Modelo}(\\mathbf{\\theta}_{\\text{hiper}})$$\n",
    "\n",
    "$$\\mathbf{\\theta}_{\\text{opt}} = \\text{modelo.fit}(\\mathbf{X}_{\\text{train}}, \\mathbf{y}_{\\text{train}})$$\n",
    "\n",
    "$$\\hat{y} = \\text{modelo.predict}(\\mathbf{x}_{\\text{novo}})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade:** \n",
    "\n",
    "Isto que foi apresentado é a essência do machine learning para classificação e regressão. Tente explicar, com a formulação matemática adequada, os seguintes conceitos:\n",
    "\n",
    "- Amostra\n",
    "\n",
    "- Modelo\n",
    "\n",
    "- RMSE\n",
    "\n",
    "- Treinamento\n",
    "\n",
    "- Predição\n",
    "\n",
    "O objetivo é conseguir explicar e escrever as fórmulas correspondentes SEM PRECISAR OLHAR NO MATERIAL DE SUPORTE. Estude o material de suporte o quanto quiser, feche o notebook, e responda a atividade no papel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em um modelo de regressão linear, desejamos obter uma predição de valor $\\hat{y}$ como uma combinação linear dos valores das *features* mais um termo de *offset*.\n",
    "\n",
    "$$\\mathbf{\\theta} = (\\theta_0, \\theta_1, \\cdots, \\theta_n)$$\n",
    "\n",
    "$$\\hat{y} = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\cdots + \\theta_n x_n$$\n",
    "\n",
    "Em notação matricial:\n",
    "\n",
    "$$\\hat{y} = \\mathbf{x}^{T} \\mathbf{\\theta}$$\n",
    "\n",
    "onde o vetor $\\mathbf{x}$ de características da amostra é formado pelas características originais e por uma \"característica extra\" que é o valor $1$. Ou seja:\n",
    "\n",
    "$$\\mathbf{x} = \\left[\n",
    "\\begin{matrix}\n",
    "1 \\\\\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "\\vdots \\\\\n",
    "x_n\n",
    "\\end{matrix}\n",
    "\\right]$$\n",
    "\n",
    "Para um dado conjunto de treinamento, podemos representar as várias quantidades deste problema em uma notação matricial:\n",
    "\n",
    "$$\n",
    "\\mathbf{X}_{\\text{train}} = \\left[\n",
    "\\begin{matrix}\n",
    "\\left(\\mathbf{x}_{\\text{train}}^{(1)}\\right)^{T} \\\\\n",
    "\\left(\\mathbf{x}_{\\text{train}}^{(2)}\\right)^{T} \\\\\n",
    "\\vdots \\\\\n",
    "\\left(\\mathbf{x}_{\\text{train}}^{(m)}\\right)^{T}\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "\\  \\text{,} \\quad\n",
    "\\mathbf{y}_{\\text{train}} = \\left[\n",
    "\\begin{matrix}\n",
    "y_{\\text{train}}^{(1)} \\\\\n",
    "y_{\\text{train}}^{(2)} \\\\\n",
    "\\vdots \\\\\n",
    "y_{\\text{train}}^{(m)} \\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "\\  \\text{,} \\quad\n",
    "\\mathbf{\\theta} = \\left[\n",
    "\\begin{matrix}\n",
    "\\theta_0 \\\\\n",
    "\\theta_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\theta_n\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "\\  \\text{,} \\quad\n",
    "\\hat{\\mathbf{y}} = \\left[\n",
    "\\begin{matrix}\n",
    "\\hat{y}^{(1)} \\\\\n",
    "\\hat{y}^{(2)} \\\\\n",
    "\\vdots \\\\\n",
    "\\hat{y}^{(m)}\n",
    "\\end{matrix}\n",
    "\\right] = \\mathbf{X}_{\\text{train}} \\mathbf{\\theta}\n",
    "$$\n",
    "\n",
    "Note que $\\mathbf{X}_{\\text{train}} \\in \\mathcal{M}_{m \\times (n + 1)}(\\mathbb{R})$ por conta da adição da coluna de \"uns\", que é necessária para acomodar o vetor de parâmetros $\\mathbf{\\theta} \\in \\mathcal{M}_{(n+1) \\times 1}(\\mathbb{R})$ que tem $(n+1)$ componentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Erro quadrático"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para um conjunto de amostras de treinamento, o erro quadrático médio é dado por:\n",
    "\n",
    "$$\n",
    "\\varepsilon^2\\left(\\mathbf{\\theta}; \\mathbf{X}_{\\text{train}}, \\mathbf{y}_{\\text{train}}\\right) = \n",
    "\\frac{1}{m} \n",
    "  \\sum_{i=1}^{m} \\left(\n",
    "    h_{\\mathbf{\\theta}}\\left(\\mathbf{x}_{\\text{train}}^{(i)}\\right) - y_{\\text{train}}^{(i)}\n",
    "  \\right)^2 =\n",
    "\\frac{1}{m} \n",
    "  \\sum_{i=1}^{m} \\left(\n",
    "    (\\mathbf{x}_{\\text{train}}^{(i)})^{T} \\mathbf{\\theta} - y_{\\text{train}}^{(i)}\n",
    "  \\right)^2 \n",
    "$$\n",
    "\n",
    "Em notação matricial,\n",
    "\n",
    "$$\n",
    "\\varepsilon^2\\left(\\mathbf{\\theta}; \\mathbf{X}_{\\text{train}}, \\mathbf{y}_{\\text{train}}\\right) = \\\\\n",
    "\\frac{1}{m} (\\mathbf{X}_{\\text{train}} \\mathbf{\\theta} - \\mathbf{y}_{\\text{train}})^T \n",
    "            (\\mathbf{X}_{\\text{train}} \\mathbf{\\theta} - \\mathbf{y}_{\\text{train}}) = \\\\\n",
    "\\frac{1}{m} (\\mathbf{\\theta}^{T} \\mathbf{X}_{\\text{train}}^{T} - \\mathbf{y}_{\\text{train}}^{T})\n",
    "            (\\mathbf{X}_{\\text{train}} \\mathbf{\\theta} - \\mathbf{y}_{\\text{train}}) = \\\\\n",
    "\\frac{1}{m} (\n",
    "    \\mathbf{\\theta}^{T} \\mathbf{X}_{\\text{train}}^{T} \\mathbf{X}_{\\text{train}} \\mathbf{\\theta} \n",
    "  - \\mathbf{\\theta}^{T} \\mathbf{X}_{\\text{train}}^{T} \\mathbf{y}_{\\text{train}}\n",
    "  - \\mathbf{y}_{\\text{train}}^{T} \\mathbf{X}_{\\text{train}} \\mathbf{\\theta}\n",
    "  + \\mathbf{y}_{\\text{train}}^{T} \\mathbf{y}_{\\text{train}}) = \\\\\n",
    "\\frac{1}{m} (\n",
    "    \\mathbf{\\theta}^{T} \\mathbf{X}_{\\text{train}}^{T} \\mathbf{X}_{\\text{train}} \\mathbf{\\theta} \n",
    "  - 2 \\mathbf{\\theta}^{T} \\mathbf{X}_{\\text{train}}^{T} \\mathbf{y}_{\\text{train}}\n",
    "  + \\mathbf{y}_{\\text{train}}^{T} \\mathbf{y}_{\\text{train}})\n",
    "$$\n",
    "\n",
    "Note que se $\\mathbf{X}_{\\text{train}}$ e $\\mathbf{y}_{\\text{train}}$ são quantidades fixas, a única variável aqui é $\\mathbf{\\theta}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cálculo matricial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/Matrix_calculus\n",
    "\n",
    "Quando temos que obter gradientes de expressões matriciais existem algumas fórmulas muito úteis do chamado *cálculo matricial*:\n",
    "\n",
    "1. Gradiente de $f(\\mathbf{x}) = \\mathbf{x}^{T} \\mathbf{a}$ onde $\\mathbf{x}$ e $\\mathbf{a}$ são matrizes-coluna:\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}) = \\mathbf{x}^{T} \\mathbf{a} \\Rightarrow \\nabla f(\\mathbf{x}) = \\mathbf{a}\n",
    "$$\n",
    "\n",
    "2. Gradiente de $f(\\mathbf{x}) = \\mathbf{x}^{T} \\mathbf{A} \\mathbf{x}$ onde $\\mathbf{x} \\in \\mathcal{M}_{n \\times 1}(\\mathbb{R})$ é matriz-coluna e $\\mathbf{A} \\in \\mathcal{M}_{n \\times n}(\\mathbb{R})$ é uma matriz quadrada:\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}) = \\mathbf{x}^{T} \\mathbf{A} \\mathbf{x} \\Rightarrow \\nabla f(\\mathbf{x}) = \\left( \\mathbf{A} + \\mathbf{A}^{T} \\right) \\mathbf{x}\n",
    "$$\n",
    "\n",
    "Caso particular: se $\\mathbf{A}$ for simétrica (ou seja, igual à sua transposta), então $\\mathbf{A} + \\mathbf{A}^{T} = 2 \\mathbf{A}$, logo:\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}) = \\mathbf{x}^{T} \\mathbf{A} \\mathbf{x} \\Rightarrow \\nabla f(\\mathbf{x}) = 2 \\mathbf{A} \\mathbf{x}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Gradiente do erro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraindo as derivadas parciais da função $\\varepsilon^2\\left(\\mathbf{X}_{\\text{train}}, \\mathbf{y}_{\\text{train}}, \\mathbf{\\theta}\\right)$ em relação aos termos $\\theta_i$ (usando o cálculo matricial) temos:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\mathbf{\\theta}} \\varepsilon^2\\left(\\mathbf{X}_{\\text{train}}, \\mathbf{y}_{\\text{train}}, \\mathbf{\\theta}\\right) = \n",
    "\\frac{1}{m} (\n",
    "    2 \\mathbf{X}_{\\text{train}}^{T} \\mathbf{X}_{\\text{train}} \\mathbf{\\theta} \n",
    "  - 2 \\mathbf{X}_{\\text{train}}^{T} \\mathbf{y}_{\\text{train}})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Equação Normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nosso objetivo é achar o vetor de parâmetros $\\mathbf{\\theta}$ que minimiza $\\varepsilon^2\\left(\\mathbf{X}_{\\text{train}}, \\mathbf{y}_{\\text{train}}, \\mathbf{\\theta}\\right)$. Para tanto, vamos resolver a equação \n",
    "\n",
    "$$\\nabla_{\\mathbf{\\theta}} \\varepsilon^2\\left(\\mathbf{X}_{\\text{train}}, \\mathbf{y}_{\\text{train}}, \\mathbf{\\theta}\\right) = \\mathbf{0}$$\n",
    "\n",
    "onde $\\mathbf{0} \\in \\mathcal{M}_{m \\times 1}(\\mathbb{R})$ é uma matriz-coluna de $m$ zeros.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "& \\nabla_{\\mathbf{\\theta}} \\varepsilon^2\\left(\\mathbf{X}_{\\text{train}}, \\mathbf{y}_{\\text{train}}, \\mathbf{\\theta}\\right) = \\mathbf{0} \\\\\n",
    "\\Leftrightarrow \\; & \\frac{1}{m} (\n",
    "    2 \\mathbf{X}_{\\text{train}}^{T} \\mathbf{X}_{\\text{train}} \\mathbf{\\theta} \n",
    "  - 2 \\mathbf{X}_{\\text{train}}^{T} \\mathbf{y}_{\\text{train}}) = \\mathbf{0} \\\\\n",
    "\\Leftrightarrow \\; &\n",
    "  \\mathbf{X}_{\\text{train}}^{T} \\mathbf{X}_{\\text{train}} \\mathbf{\\theta} =\n",
    "  \\mathbf{X}_{\\text{train}}^{T} \\mathbf{y}_{\\text{train}} \\\\\n",
    "\\Leftrightarrow \\; &\n",
    "  \\mathbf{\\theta} = (\\mathbf{X}_{\\text{train}}^{T} \\mathbf{X}_{\\text{train}})^{-1}\n",
    "  \\mathbf{X}_{\\text{train}}^{T} \\mathbf{y}_{\\text{train}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Esta última linha é conhecida como **equação normal** do problema de regressão linear. Será que corresponde a um ponto de mínimo? Calculando a segunda derivada do erro quadrático temos o *Hessiano* de $\\varepsilon^2\\left(\\mathbf{X}_{\\text{train}}, \\mathbf{y}_{\\text{train}}, \\mathbf{\\theta}\\right)$:\n",
    "\n",
    "$$\n",
    "\\mathbf{H}_{\\mathbf{\\theta}}\\left(\\mathbf{X}_{\\text{train}}, \\mathbf{y}_{\\text{train}}, \\mathbf{\\theta}\\right) = \\frac{2}{m} \\mathbf{X}_{\\text{train}}^{T} \\mathbf{X}_{\\text{train}}\n",
    "$$\n",
    "\n",
    "Como a matriz resultante é positiva-definida (https://en.wikipedia.org/wiki/Positive-definite_matrix), temos a segurança de que a resposta da equação normal é realmente um ponto de mínimo!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usando a equação normal para treinar uma regressão linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chegamos finalmente ao nosso primeiro algoritmo de treinamento do curso de machine learning! Vamos treinar um modelo de regressão linear usando a equação normal.\n",
    "\n",
    "A vantagem de se usar a equação normal é que a solução final é obtida diretamente por uma fórmula. A desvantagem é que o cálculo desta fórmula não escala bem com o número de *features* do problema, porque a matriz quadrada $\\mathbf{X}_{\\text{train}}^{T} \\mathbf{X}_{\\text{train}}$ deve ser invertida e esta tem tamanho $n \\times n$. Se o número de *features* for muito alto (e.g. pixels em uma imagem) o custo desta operação é alto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade:** Qual a complexidade computacional de se inverter uma matriz?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade:** Qual a complexidade computacional do treinamento do regressor linear via equação normal, em termos de:\n",
    "\n",
    "- $n$: número de *features*, e\n",
    "- $m$: número de amostras de treinamento?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade:** Em função dos resultados acima, você recomendaria o uso da equação normal em um problema:\n",
    "\n",
    "1. Com poucas amostras de treinamento e muitas *features*?\n",
    "\n",
    "2. Com muitas amostras de treinamento e poucas *features*?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R:** Não recomedaria muitas features, a complexidade aumenta exponencialmente com o aumento de features. Mas recomendaria poucas features e muitas amostras, pois o numero de amostras aumenta linearmente com o aumento do número de amostras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos gerar alguns pontos de dados para testar nossa regressão linear. Nossa função linear real será $f(x) = 3 x + 4$ e á esta adicionaremos ruído gaussiano de média zero e desvio padrão unitário:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 100\n",
    "\n",
    "X = 2 * np.random.rand(m, 1)  # Gera m pontos aleatórios entre 0 e 2.\n",
    "ruido = np.random.randn(m, 1)  # Gera um ruido gaussiano de desvio padrão 1.0\n",
    "\n",
    "y = 4 + 3 * X + ruido  # Nosso sinal detectado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pergunta:** Quais são os parâmetros ótimos esperados ao se treinar um modelo de regressão linear sobre dados gerados por esta código?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_aux = np.array([[0.0], [2.0]])\n",
    "y_aux = 4 + 3 * X_aux\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.plot(X_aux, y_aux, \"r--\")\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.axis([0, 2, 0, 15])\n",
    "plt.title('Amostras aleatórias e curva ideal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para usar a equação normal temos que aumentar a matriz de amostras de treinamento com uma coluna extra de valores $1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho original da matriz de amostras: (100, 1)\n",
      "Algumas das amostras originais:\n",
      "[[0.00723172]\n",
      " [0.21947489]\n",
      " [0.70170069]\n",
      " [0.16720456]\n",
      " [1.69180333]]\n"
     ]
    }
   ],
   "source": [
    "print(f'Tamanho original da matriz de amostras: {X.shape}')\n",
    "print(f'Algumas das amostras originais:\\n{X[:5]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho da matriz de amostras aumentada: (100, 2)\n",
      "Algumas das amostras da matriz aumentada:\n",
      "[[1.         0.00723172]\n",
      " [1.         0.21947489]\n",
      " [1.         0.70170069]\n",
      " [1.         0.16720456]\n",
      " [1.         1.69180333]]\n"
     ]
    }
   ],
   "source": [
    "# Adicionando uma coluna de 1s.\n",
    "X_b = np.c_[np.ones((m, 1)), X]\n",
    "\n",
    "print(f'Tamanho da matriz de amostras aumentada: {X_b.shape}')\n",
    "print(f'Algumas das amostras da matriz aumentada:\\n{X_b[:5]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos usar a equação normal para calcular os parâmetros ótimos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.24267645],\n",
       "       [2.77161497]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
    "theta_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chegamos perto do resultado ideal!\n",
    "\n",
    "**Atividade:** Mude o número de pontos de treinamento e veja se os parâmetros ótimos se aproximam dos parâmetros ideais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R**: Em geral, quanto mais amostras no modelo, mais próximo será do modelo ideal, porém, a precisão aumenta logaritmicamente com o número de pontos, o que indica que precisa de 10 vezes o número de amostras para aumentar em uma casa de precisão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos ver como o modelo se comporta para fazer predições:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um vetor de amostras fictícias de teste.\n",
    "X_test = np.linspace(0.0, 2.0, num=100).T\n",
    "\n",
    "# Calculando os valores de y segundo a equação normal.\n",
    "X_test_b = np.c_[np.ones((len(X_test), 1)), X_test]\n",
    "y_test = X_test_b.dot(theta_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(X, y, \"b.\", label='amostras')\n",
    "plt.plot(X_aux, y_aux, \"r\", label='ideal')\n",
    "plt.plot(X_test, y_test, \"g-\", label='predições')\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.legend(loc=\"upper left\", fontsize=14)\n",
    "plt.axis([0, 2, 0, 15])\n",
    "plt.title('Amostras aleatórias, curva ideal e resultado do ajuste')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos comparar nosso preditor com o regressor linear do scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.24267645]), array([[2.77161497]]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultados identicos!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade:** Escreva seu próprio regressor linear no framework do scikit-learn. Teste o regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um outro método de treinamento de uma regressão linear é o método do máximo declive (*gradient descent*). Este método também serve para outros regressores e classificadores, é bastante genérico - usaremos bastante o gradient descent em redes neurais!\n",
    "\n",
    "Este método se baseia na observação seguinte:\n",
    "\n",
    "- O gradiente da função de erro médio é um vetor no espaço de características que aponta na direção de maior crescimento do erro.\n",
    "- Logo, se desejo achar os parâmetros que minimizam o erro, basta andar na direção contrária do gradiente do erro!\n",
    "\n",
    "Ou seja, partindo de um \"chute\" aleatório $\\mathbf{\\theta}_{0}$ dos valores dos parâmetros, vamos iterativamente melhorar nossos parâmetros através de pequenos incrementos neste na direção oposta ao gradiente da função de erro:\n",
    "\n",
    "$$\n",
    "\\mathbf{\\theta}_{i + 1} = \\mathbf{\\theta}_{i} - \\eta \\nabla_{\\mathbf{\\theta}} \\varepsilon^2\\left(\\mathbf{\\theta}_{i}; \\mathbf{X}_{\\text{train}}, \\mathbf{y}_{\\text{train}}\\right)\n",
    "$$\n",
    "\n",
    "O **hiperparâmetro** $\\eta$ é chamado de *passo de aprendizado* (*learning step*) do algoritmo.\n",
    "\n",
    "Continuamos iterando através desta fórmula até que não tenhamos mais mudanças significativas em $\\mathbf{\\theta}$, ou seja: para uma pequena tolerância $\\epsilon > 0$\n",
    "\n",
    "$$\n",
    "\\left| \\, \\mathbf{\\theta}_{i + 1} - \\mathbf{\\theta}_{i} \\, \\right| < \\epsilon\n",
    "$$\n",
    "\n",
    "O **hiperparâmetro** $\\epsilon$ controla a precisão desejada na solução final. Se a tolerância for muito pequena pode ser que o algoritmo não convirja rapidamente. Neste caso é comum definir um número máximo de iterações através de outro **hiperparâmetro** ``max_iter``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade:** Estude o material do livro texto (Géron), capítulo 4 e responda:\n",
    "\n",
    "- Qual a diferença entre batch gradient descent, stochastic gradient descent, e mini-batch gradient descent?\n",
    "- O que acontece quando $\\eta$ é muito pequeno?\n",
    "- O que acontece quando $\\eta$ é muito grande?\n",
    "- Quais as classes do scikit-learn que implementam regressão linear usando a equação normal no treinamento? Quais usam gradient descent em alguma de suas formas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos implementar o método do *gradient descent* para resolver o problema do treinamento do modelo linear.\n",
    "\n",
    "Vamos desenhar a discussão acima na forma de pseudo-código:\n",
    "\n",
    "- Entrada: \n",
    "    - Conjunto de treinamento $(\\mathbf{X}_{m \\times n}, \\mathbf{y}_{m \\times 1})$\n",
    "    - Valores iniciais do vetor de parâmetros $\\theta$\n",
    "    - Hiperparâmetros do algoritmo: \n",
    "        - tolerância $\\epsilon$\n",
    "        - número máximo de iterações $N_{\\text{max}}$\n",
    "        - taxa de aprendizado $\\eta$\n",
    "- Saida: \n",
    "    - Valores ótimos do vetor de parametros $\\theta$\n",
    "\n",
    "Algoritmo:\n",
    "\n",
    "- $\\text{acabou} \\leftarrow \\text{Falso}$\n",
    "- $\\text{iter} \\leftarrow 0$\n",
    "- enquanto não $\\text{acabou}$:\n",
    "    - $\\theta_{\\text{novo}} \\leftarrow \\theta - \\eta \\nabla_{\\theta} \\varepsilon^2\\left(\\theta; \\mathbf{X}, \\mathbf{y}\\right)$\n",
    "    - $\\text{iter} \\leftarrow \\text{iter} + 1$\n",
    "    - se $\\left| \\, \\theta - \\theta_{\\text{novo}} \\, \\right| < \\epsilon$ ou $\\text{iter} > N_{\\text{max}}$:\n",
    "        - $\\text{acabou} \\leftarrow \\text{Verdadeiro}$\n",
    "    - $\\theta \\leftarrow \\theta_{\\text{novo}}$\n",
    "- retorna $\\theta$\n",
    "\n",
    "**Atividade**: Implemente o algoritmo de *gradient descent* na forma de um modelo do Scikit-Learn e teste o resultado! Lembrando: \n",
    "\n",
    "$$\n",
    "\\nabla_{\\theta} \\varepsilon^2\\left(\\theta ; \\mathbf{X}, \\mathbf{y}\\right) = \n",
    "\\frac{1}{m} (\n",
    "    2 \\mathbf{X}^{T} \\mathbf{X} \\mathbf{\\theta} \n",
    "  - 2 \\mathbf{X}^{T} \\mathbf{y})\n",
    "$$\n",
    "\n",
    "sendo que a matriz $\\mathbf{X}$ já contem o termo constante (aquela coluna de $1$s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch versus mini-batch versus stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O algoritmo acima chama-se *batch gradient descent*, pois opera com o lote (*batch*) completo de amostras de treinamento de uma vez só. Mas pense um pouco: será que precisamos de todas as amostras de treinamento para calcular um gradiente razoável? Considere os cenários a seguir:\n",
    "\n",
    "- Calcular o gradiente do erro usando todas as amostras de um conjunto de treinamento com centenas de milhares de amostras.\n",
    "\n",
    "- Calcular o gradiente do erro usando metade deste mesmo conjunto de treinamento.\n",
    "\n",
    "Certamente o segundo caso não vai retornar o \"melhor\" gradiente, mas vai retornar algo muito próximo com um custo *MUITO* menor! Outra consideração, até mais importante: *O CONJUNTO DE TREINAMENTO PODE NEM CABER NA MEMÓRIA RAM*!\n",
    "\n",
    "Este *insight* leva ao método *mini-batch gradient descent*: ao invés de trabalhar com o conjunto completo de treinamento de uma vez só, trabalhe com ele por pedaços, rotacionando o pedaço usado a cada iteração do algoritmo.\n",
    "\n",
    "O algoritmo então fica assim:\n",
    "\n",
    "- $acabou \\leftarrow Falso$\n",
    "- $epoch \\leftarrow 0$\n",
    "- enquanto não $acabou$:\n",
    "    - $\\theta_{novo} \\leftarrow \\theta$\n",
    "    - Para cada *mini-batch* $(\\mathbf{X}_{b}, \\mathbf{y}_{b})$ de $(\\mathbf{X}, \\mathbf{y})$:\n",
    "        - $\\theta_{novo} \\leftarrow \\theta_{novo} - \\eta \\nabla_{\\theta_{novo}} \\varepsilon^2\\left(\\mathbf{X}_{b}, \\mathbf{y}_{b}, \\theta_{novo}\\right)$\n",
    "    - $epoch \\leftarrow epoch + 1$\n",
    "    - se $\\left| \\, \\theta - \\theta_{novo} \\, \\right| < \\epsilon$ ou $epoch > N_{\\text{max}}$:\n",
    "        - $acabou \\leftarrow Verdadeiro$\n",
    "    - $\\theta \\leftarrow \\theta_{novo}$\n",
    "- retorna $\\theta$\n",
    "\n",
    "Cada passagem completa por todos os *mini-batches* (ou seja, um passo pelo *batch* completo) chama-se *epoch* no linguajar de machine learning.\n",
    "\n",
    "No caso limite em que o tamanho do *mini-batch* é $1$ (!!!), mas a amostra é escolhida ao acaso (ao invés de passar por todas as amostras), temos o algoritmo *stochastic gradient descent*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Regressão polinomial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E se os dados não se ajustam a uma linha reta (ou um hiperplano, no caso geral)? Considere o seguinte problema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Vamos gerar m = 100 amostras de exemplo.\n",
    "m = 100\n",
    "\n",
    "# As coordenadas x são geradas aleatoriamente entre -3 e 3.\n",
    "X = 6 * np.random.rand(m, 1) - 3\n",
    "\n",
    "# Computando os valores de y\n",
    "y = 0.5 * X**2 + X + 2\n",
    "\n",
    "# Adicionando ruido.\n",
    "y += 1.0 * np.random.randn(m, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.axis([-3, 3, 0, 10])\n",
    "plt.title('Amostras aleatórias')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviamente não podemos atingir um bom *fit* com uma linha reta! A solução para nosso problema é criar novas *features* com valores polinomiais:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Tamanho original da matriz de amostras: {X.shape}')\n",
    "print(f'Algumas das amostras originais:\\n{X[:5]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicionando uma coluna de 1s e uma coluna de x**2.\n",
    "X_b = np.c_[np.ones((m, 1)), X, X**2]\n",
    "\n",
    "print(f'Tamanho da matriz de amostras aumentada: {X_b.shape}')\n",
    "print(f'Algumas das amostras da matriz aumentada:\\n{X_b[:5]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos novamente a equação normal para achar os parâmetros ótimos (ou o gradient descent também serve!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
    "theta_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos ver como ficou nosso *fit*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.linspace(-3, 3, num=100).T\n",
    "X_test_b = np.c_[np.ones((m, 1)), X_test, X_test**2]\n",
    "y_test = X_test_b.dot(theta_best)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.plot(X_test, y_test, \"r-\")\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.axis([-3, 3, 0, 10])\n",
    "plt.title('Ajuste de modelo quadrático')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para criar *features* polinomiais de um grau qualquer em scikit-learn podemos usar a classe ``PolynomialFeatures``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Nota: include_bias=False porque o bias (termo constante) já estará incluso\n",
    "# no regressor linear.\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly_features.fit_transform(X)\n",
    "\n",
    "print(X.shape)\n",
    "print(X_poly.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X[0])\n",
    "print(X_poly[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos então fazer um regressor linear com estes novos dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_poly, y)\n",
    "lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare os resultados obtidos aqui com aqueles da equação normal obtidos anteriormente.\n",
    "\n",
    "Assim como o uso de *features* polinomiais leva a um modelo com melhor ajuste, podemos também usar outras funções como *features*, tais como $\\text{log}()$, $\\text{sin}()$, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting versus underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O que acontece se o grau do *fit* polinomial for muito baixo? E se for muito alto? Vamos experimentar um pouco:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(X, y, \"b.\", linewidth=3)\n",
    "plt.title('Amostras aleatórias')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model(X_test, y_test, degree):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(X, y, \"b.\", linewidth=3)\n",
    "    plt.plot(X_test, y_test, 'r-', label=f'grau {degree}')\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "    plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "    plt.axis([-3, 3, 0, 10])\n",
    "    plt.title(f'Modelo de grau {degree}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimento 1: grau baixo.\n",
    "poly_reg_1 = Pipeline([\n",
    "    (\"poly_features\", PolynomialFeatures(degree=1, include_bias=False)),\n",
    "    (\"std_scaler\", StandardScaler()),\n",
    "    (\"lin_reg\", LinearRegression()),\n",
    "])\n",
    "poly_reg_1.fit(X, y)\n",
    "y_test_1 = poly_reg_1.predict(X_test.reshape(-1, 1))\n",
    "\n",
    "plot_model(X_test, y_test_1, 1)\n",
    "\n",
    "# Experimento 2: grau adequado.\n",
    "poly_reg_2 = Pipeline([\n",
    "    (\"poly_features\", PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    (\"std_scaler\", StandardScaler()),\n",
    "    (\"lin_reg\", LinearRegression()),\n",
    "])\n",
    "poly_reg_2.fit(X, y)\n",
    "y_test_2 = poly_reg_2.predict(X_test.reshape(-1, 1))\n",
    "\n",
    "plot_model(X_test, y_test_2, 2)\n",
    "\n",
    "# Experimento 3: grau muito alto.\n",
    "poly_reg_30 = Pipeline([\n",
    "    (\"poly_features\", PolynomialFeatures(degree=30, include_bias=False)),\n",
    "    (\"std_scaler\", StandardScaler()),\n",
    "    (\"lin_reg\", LinearRegression()),\n",
    "])\n",
    "poly_reg_30.fit(X, y)\n",
    "y_test_30 = poly_reg_30.predict(X_test.reshape(-1, 1))\n",
    "\n",
    "plot_model(X_test, y_test_30, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(X, y, \"b.\", linewidth=3)\n",
    "plt.plot(X_test, y_test_1, 'r-', label='grau 1')\n",
    "plt.plot(X_test, y_test_2, 'b-', label='grau 2')\n",
    "plt.plot(X_test, y_test_30, 'k-', label='grau 30')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.axis([-3, 3, 0, 10])\n",
    "plt.title('Todos os modelos')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A curva preta, correspondente ao *fit* de grau 30, apresenta *overfitting*. Ao tentar se ajustar o melhor possível aos pontos de dados, a curva tende a apresentar comportamento exagerado fora destes. \n",
    "\n",
    "Uma característica marcante do *overfitting* é que o sistema perde sua capacidade de fazer predições de boa qualidade quando não está na vizinhança de um ponto de treinamento. Dizemos que o sistema não **generaliza** bem. Para observar a capacidade de generalização de um sistema podemos usar a estratégia de validação cruzada e obter uma medida mais realista do RMSE do classificador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "def experiment(reg, X, y, degree):\n",
    "    scores = np.sqrt(-cross_val_score(\n",
    "        reg,\n",
    "        X,\n",
    "        y,\n",
    "        cv=5,\n",
    "        scoring='neg_mean_squared_error',\n",
    "    ))\n",
    "    print(f'Fit de grau {degree}:')\n",
    "    print(f'RMSE = {scores}')\n",
    "    print(f'média = {scores.mean()}, std = {scores.std()}')\n",
    "\n",
    "\n",
    "experiments = (\n",
    "    (1, poly_reg_1),\n",
    "    (2, poly_reg_2),\n",
    "    (30, poly_reg_30),\n",
    ")\n",
    "\n",
    "for degree, regressor in experiments:\n",
    "    experiment(regressor, X, y, degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que o erro de treinamento conta uma história diferente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "def final_train_error(X, y, reg):\n",
    "    reg.fit(X, y)\n",
    "    y_pred = reg.predict(X)  # Sim, estou predizendo no conjunto de treinamento.\n",
    "    return np.sqrt(mean_squared_error(y_pred, y))\n",
    "\n",
    "\n",
    "def params_to_str(pipeline):\n",
    "    aux = pipeline.named_steps['lin_reg']\n",
    "    return 'intercept = {}, coefs = {}'.format(aux.intercept_, aux.coef_)\n",
    "\n",
    "\n",
    "for degree, regressor in experiments:\n",
    "    train_error = final_train_error(X, y, regressor)\n",
    "    print(f'Fit de grau {degree}: RMSE = {train_error}')\n",
    "\n",
    "for degree, regressor in experiments:\n",
    "    print(f'Modelo de grau {degree}: {params_to_str(regressor)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos aqui observar uma característica marcante do *overfitting* versus *underfitting*:\n",
    "\n",
    "- **overfitting**: \n",
    "    - O modelo é **complexo demais** (permite sobreajuste, ou *overfitting*)\n",
    "    - erro de treinamento **baixo**\n",
    "    - erro médio de teste **geralmente alto** (mas **ocasionalmente baixo**)\n",
    "    - **variância alta** do erro de teste\n",
    "    \n",
    "- **underfitting**:\n",
    "    - O modelo é **simples demais** (não permite bom ajuste)\n",
    "    - erro de treinamento **alto**\n",
    "    - erro de teste **consistentemente alto**\n",
    "    - **variância baixa** do erro de teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regressor ótimo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E agora, um pouco de Matemática!\n",
    "\n",
    "Para um dado valor de $\\mathbf{x}$, qual é o valor ideal de $h(\\mathbf{x})$? Depende do que a gente define por \"ideal\". \n",
    "\n",
    "Vamos supor que a definição de \"ideal\" é dada pelo RMSE. Minimizar o RMSE é a mesma coisa que minimizar o MSE (Minimum Squared Error):\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{m} \\sum_{i=1}^{m}\\left(h(\\mathbf{x}_i) - \\mathbf{y}_i\\right)^{2}\n",
    "$$\n",
    "\n",
    "Para atingir o mínimo do MSE devemos escolher uma $h(\\mathbf{x})$ que minimize $(h(\\mathbf{x}) - y)^2$ para qualquer valor dado de $\\mathbf{x}$. Mas veja só o problema: se para um dado $\\mathbf{x}$ só tivéssemos uma única possibilidade para $y$, o nosso problema estaria resolvido! Bastaria fazer o $h(\\mathbf{x})$ valer aquele $y$ e pronto! Só que o mundo real tem incertezas, ruído e variabilidade na aquisição de nossos vários $\\mathbf{x}_i$ e $y_i$, então não podemos afirmar categoricamente que para cada $\\mathbf{x}$ só existirá um único $y$, eterno e imutável...\n",
    "\n",
    "Pense assim: agora observamos que para um certo vetor $\\mathbf{x}$ medido, também medimos um valor $y$. Só que na sequência medimos de novo um valor de $y$ para esse mesmo $\\mathbf{x}$ e dá um valor um pouquinho diferente, por conta do ruído e outras incertezas!\n",
    "\n",
    "Matematicamente, dizemos que cada uma de nossas amostras $(\\mathbf{x}_i, y_i)$ foi obtida de um vetor aleatório (ou seja, um vetor de variáveis aleatórias) $(X, Y)$. Quando dizemos que existe incerteza em $y$ para um dado $\\mathbf{x}$, estamos dizendo que existe uma distribuição aleatória de valores de $y$ para este valor de $\\mathbf{x}$, ou seja, existe uma distribuição $P(Y | X=\\mathbf{x})$.\n",
    "\n",
    "Por exemplo, na figura a seguir temos uma situação em que foram observados os pontos em vermelho. Estes pontos foram gerados artificialmente assim:\n",
    "\n",
    "- Foram escolhidos pontos $x$ ao acaso entre $0$ e $10$\n",
    "\n",
    "- Para cada $x$ obtem-se um $y$ a partir de um polinomio de terceiro grau (curva em azul) ao qual se adicionou um valor aleatório amostrado de uma distribuição normal $N(\\mu=0, \\sigma=0.5)$.\n",
    "\n",
    "- Os pontos assim obtidos estão desenhados em vermelho.\n",
    "\n",
    "São dados *fake*, só para a gente estudar esse problema.\n",
    "\n",
    "![Ilustração P(Y | X=x)](y_for_x.png \"Ilustração P(Y | X=x)\")\n",
    "\n",
    "(Curiosamente, parece que a curva em azul é um excelente interpolador, ou seja, um candidato ideal para $h(\\mathbf{x})$! Isso não é um acidente, e vamos provar isso logo a seguir!)\n",
    "\n",
    "Vamos nos concentrar no valor $x = 6.2$, para efeito de exemplo. Qual a distribuição dos valores de $y$? Conforme visto no nosso processo artificial de geração dos pontos de exemplo, esperamos que a distribuição dos pontos de $y$ seja uma Gaussiana com média igual ao valor do polinômio azul ($4.48$), e desvio padrão valendo $0.5$. Isto está ilustrado na figura da direita.\n",
    "\n",
    "Então a frase \n",
    "\n",
    "- \"devemos minimizar $(h(\\mathbf{x}) - y)^2$\" \n",
    "\n",
    "não faz sentido (pois $y$ não é um valor único, é uma distribuição de valores), mas sim \n",
    "\n",
    "- \"devemos minimizar $E_{Y | X = \\mathbf{x}}[(h(X) - Y)^2]$\"\n",
    "\n",
    "onde o subscrito $Y | X = \\mathbf{x}$ no operador de esperança indica que estamos obtendo o valor esperado da expressão $(h(X) - Y)^2$ em relação à variável aleatória $Y$, no caso em que a variável aleatória $X$ vale $\\mathbf{x}$ (neste exemplo, $\\mathbf{x} = 6.2$).\n",
    "\n",
    "Para simplificar a notação, vamos chamar de $\\hat{y} = h(\\mathbf{x})$ o valor estimado de $y$ em $\\mathbf{x}$. Vamos também chamar de $\\overline{y}$ o valor esperado de $Y$ dado $X = \\mathbf{x}$, ou seja, $\\overline{y} = E_{Y | X = \\mathbf{x}}[Y]$.\n",
    "\n",
    "Temos o seguinte:\n",
    "\n",
    "\\begin{align*}\n",
    "E_{Y | X = \\mathbf{x}}[(h(X) - Y)^2] & = E_{Y | X = \\mathbf{x}}[(h(\\mathbf{x}) - Y)^2] \n",
    "& \\,\\, \\text{(Pois $X$ vale $\\mathbf{x}$)} \\\\\n",
    "& = E_{Y | X = \\mathbf{x}}[(\\hat{y} - Y)^2]\n",
    "& \\,\\, \\text{(Pois $h(\\mathbf{x}) = \\hat{y}$)} \\\\\n",
    "& = E_{Y | X = \\mathbf{x}}[((\\hat{y} - \\overline{y}) - (Y - \\overline{y}))^2]\n",
    "& \\,\\, \\text{(Subtraindo e somando $\\overline{y}$)} \\\\\n",
    "& = E_{Y | X = \\mathbf{x}}[\n",
    "    (\\hat{y} - \\overline{y})^2 \n",
    "    + (Y - \\overline{y})^2 \n",
    "    - 2 (\\hat{y} - \\overline{y}) (Y - \\overline{y})]\n",
    "& \\,\\, \\text{(Produto notável)} \\\\\n",
    "& = E_{Y | X = \\mathbf{x}}[(\\hat{y} - \\overline{y})^2] \n",
    "& \\,\\, \\text{(Linearidade da esperança)} \\\\\n",
    "& \\phantom{ = }  + E_{Y | X = \\mathbf{x}}[(Y - \\overline{y})^2] \\\\\n",
    "& \\phantom{ = }  - 2 E_{Y | X = \\mathbf{x}}[(\\hat{y} - \\overline{y}) (Y - \\overline{y})] \\\\\n",
    "& = (\\hat{y} - \\overline{y})^2 \n",
    "& \\,\\, \\text{(Pois $\\hat{y}$ e $\\overline{y}$ são constantes)} \\\\\n",
    "& \\phantom{ = }  + \\text{Var}(Y | X = \\mathbf{x})\n",
    "& \\,\\, \\text{(Definição de variância)} \\\\\n",
    "& \\phantom{ = }  - 2 (\\hat{y} - \\overline{y}) (E_{Y | X = \\mathbf{x}}[Y] - \\overline{y})\n",
    "& \\,\\, \\text{(Pois $\\hat{y}$ e $\\overline{y}$ são constantes)} \\\\\n",
    "& = (\\hat{y} - \\overline{y})^2 + \\text{Var}(Y | X = \\mathbf{x})\n",
    "& \\,\\, \\text{(Pois $E_{Y | X = \\mathbf{x}}[Y] - \\overline{y} = 0$)} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Lembre-se: nosso objetivo é definir qual o melhor valor de $\\hat{y}$. Como nada se pode fazer sobre $\\text{Var}(Y | X = \\mathbf{x})$, só nos resta minimizar o termo $(\\hat{y} - \\overline{y})^2$. Isso é trivial: basta fazer $\\hat{y} = \\overline{y}$. Lembrando que $\\hat{y} = h(\\mathbf{x})$, chegamos ao nosso resultado final:\n",
    "\n",
    "**O regressor que minimiza o valor esperado do RMSE é $h(\\mathbf{x}) = \\text{Média}(Y | X = \\mathbf{x})$**\n",
    "\n",
    "Ou seja, a melhor estimativa de $y$ em $\\mathbf{x}$ é o valor médio de $y$ naquele ponto.\n",
    "\n",
    "Lembra-se que eu disse na aula 2 que se a gente trocasse a função quadrado por módulo dava outro resultado? Pois fica aqui a afirmação (sem demonstração no momento) sobre o melhor regressor nessas condições:\n",
    "\n",
    "**O regressor que minimiza o erro absoluto médio é $h(\\mathbf{x}) = \\text{Mediana}(Y | X = \\mathbf{x})$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Overfitting* e *underfitting* versus regressor ótimo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que conhecemos o regressor ótimo, vamos compará-lo no dataset artificial acima aos regressores polinomiais de grau 1 (muito baixo), grau 3 (ideal, pois coincide com o regressor ótimo para este dataset artificial) e grau 8 (muito alto).\n",
    "\n",
    "Geramos tres datasets diferentes para treinamento e um extra para teste, todos com a mesma estratégia acima (polinômio de grau 3, ao qual adicionamos ruido gaussiano).\n",
    "\n",
    "#### Fit linear - underfitting\n",
    "\n",
    "Vemos abaixo o resultado do fit linear para estes dados. As linhas sólidas coloridas representam o fit linear, enquando a linha tracejada preta demonstra o modelo ideal (o regressor ótimo).\n",
    "\n",
    "![fit grau 1](fit_grau_1.png \"fit grau 1\")\n",
    "\n",
    "Observe que todos os regressores lineares são próximos entre si, e todos falham em se ajustar bem aos dados por não terem liberdade suficiente para fazê-lo. Esta é uma característica do *underfitting*: os regressores ficam todos parecidos, e igualmente ruins.\n",
    "\n",
    "#### Fit polinomial de grau 3 - perto do ideal\n",
    "\n",
    "Vemos abaixo o resultado do ajuste polinomial de grau 3 para estes dados.\n",
    "\n",
    "![fit grau 3](fit_grau_3.png \"fit grau 3\")\n",
    "\n",
    "Observe que todos os regressores são próximos entre si, e também próximos do regressor ideal. Esta é uma caraterística que indica que estamos próximos do modelo de regressor ideal: todos os regressores ajustados são similares, e o erro de teste médio é o menor que se pode obter.\n",
    "\n",
    "#### Fit polinomial de grau 8 - overfitting\n",
    "\n",
    "Vemos abaixo o resultado do ajuste polinomial de grau 8 para estes dados.\n",
    "\n",
    "![fit grau 1](fit_grau_8.png \"fit grau 8\")\n",
    "\n",
    "Note que os vários regressores se ajustam bem aos pontos dos conjuntos de treinamento, mas diferem bastante entre si. Isto acontece justamente porque estes regressores se ajustaram tão bem aos dados de treinamento, incorporando a variabilidade de dados que se deve ao ruído, e não ao polinômio de grau 3 subjacente.\n",
    "\n",
    "#### Comparação entre os modelos\n",
    "\n",
    "Observe um *zoom* dos gráficos acima comparando os vários modelos:\n",
    "\n",
    "![comparação](comparacao_fits.png \"comparação entre fits\")\n",
    "\n",
    "---\n",
    "\n",
    "**Atividade:**\n",
    "\n",
    "Escreva, em suas palavras, uma análise comparativa simples entre os três cenários do gráfico de comparação acima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Bias/variance tradeoff*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seja $\\mathbf{D}$ um conjunto de treinamento de nosso modelo (o bom e velho $(\\mathbf{X}_{train}, \\mathbf{y}_{train})$), com $m$ amostras de treinamento.\n",
    "\n",
    "Primeiro, observamos o seguinte: para cada possível conjunto de treinamento $\\mathbf{D}$ temos um vetor de parâmetros ótimos $\\mathbf{\\theta}_{\\mathbf{D}}$ diferente, e consequentemente um modelo ajustado $h_{\\mathbf{\\theta}_{\\mathbf{D}}}(\\mathbf{x})$ diferente. (Vimos isso acima quando obtemos regressores diferentes para conjuntos de treinamento diferentes.)\n",
    "\n",
    "Logo, para um determinado valor de $\\mathbf{x}$, temos que **$h_{\\mathbf{\\theta}_{\\mathbf{D}}}(\\mathbf{x})$ é uma variável aleatória sobre o conjunto de todos os possíveis conjuntos de treinamento $\\mathbf{D}$**.\n",
    "\n",
    "Vamos definir a média e a variância desta variável aleatória (para um dado $\\mathbf{x}$, não se esqueça) como:\n",
    "\n",
    "$$\n",
    "\\mu_{h}(\\mathbf{x}) = E_{\\mathbf{D}}\\left[h_{\\mathbf{\\theta}_{\\mathbf{D}}}(\\mathbf{x})\\right]\n",
    "$$\n",
    "\n",
    "e\n",
    "\n",
    "$$\n",
    "\\sigma_{h}^2(\\mathbf{x}) = E_{\\mathbf{D}}\\left[ \\left(\n",
    "    h_{\\mathbf{\\theta}_{\\mathbf{D}}}(\\mathbf{x}) - \\mu_{h}(\\mathbf{x})\n",
    "\\right)^2 \\right]\n",
    "$$\n",
    "\n",
    "Respire, respire...\n",
    "\n",
    "Agora vamos aos dados: vamos imaginar que nossos dados *observados* são da seguinte forma:\n",
    "\n",
    "$$\n",
    "(\\text{valor observado}) = (\\text{valor real}) + (\\text{ruído e outros erros})\n",
    "$$\n",
    "\n",
    "Dando nomes aos bois, seja $y(\\mathbf{x})$ o valor observado da variável dependente para uma amostra de entrada $\\mathbf{x}$, $t(\\mathbf{x})$ o valor real (true) da variável dependente, e $\\varepsilon$ o ruído, que é uma variável aleatória independente de média zero e variância $\\sigma_{\\varepsilon}^2 = E\\left[ \\varepsilon^2 \\right]$. Então a pseudo-equação acima fica sendo:\n",
    "\n",
    "$$\n",
    "y(\\mathbf{x}) = t(\\mathbf{x}) + \\varepsilon\n",
    "$$\n",
    "\n",
    "Respire, respire...\n",
    "\n",
    "Agora vamos juntar tudo. O erro entre o valor observado e o valor predito é\n",
    "\n",
    "$$\n",
    "y(\\mathbf{x}) - h_{\\mathbf{\\theta}_{\\mathbf{D}}}(\\mathbf{x})\n",
    "$$\n",
    "\n",
    "O erro quadrático é \n",
    "\n",
    "$$\n",
    "\\left(y(\\mathbf{x}) - h_{\\mathbf{\\theta}_{\\mathbf{D}}}(\\mathbf{x})\\right)^2\n",
    "$$\n",
    "\n",
    "e - leia com calma - *o erro médio quadrático sobre todas as amostras de teste possíveis* ***e*** *para um dado conjunto de treinamento* é, portanto:\n",
    "\n",
    "$$\n",
    "\\text{MSE}_{\\mathbf{D}} = E_{\\mathbf{x}}\\left[\\left(y(\\mathbf{x}) - h_{\\mathbf{\\theta}_{\\mathbf{D}}}(\\mathbf{x})\\right)^2\\right]\n",
    "$$\n",
    "\n",
    "Ainda sobrou uma incerteza no processo: o conjunto de treinamento $\\mathbf{D}$. Nossa estimativa final de erro é o *valor esperado do erro quadrático médio $\\text{MSE}_{\\mathbf{D}}$ sobre todos os possíveis conjuntos de treinamento*, ou seja:\n",
    "\n",
    "$$\n",
    "\\text{MSE} = E_{\\mathbf{D}} \\left[ E_{\\mathbf{x}} \\left[ \n",
    "\\left(y(\\mathbf{x}) - h_{\\mathbf{\\theta}_{\\mathbf{D}}}(\\mathbf{x})\\right)^2\n",
    "\\right] \\right]\n",
    "$$\n",
    "\n",
    "Podemos demonstrar que essa expressão pode ser escrita como:\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \n",
    "\\underbrace{E_{\\mathbf{x}} \\left[ \\left( t(\\mathbf{x}) - \\mu_{h}(\\mathbf{x}) \\right)^2 \\right]}_{\\text{bias}}\n",
    "+ \\underbrace{E_{\\mathbf{x}} \\left[ \\sigma_{h}^2(\\mathbf{x}) \\right] }_{\\text{variance}}\n",
    "+ \\underbrace{\\sigma_{\\varepsilon}^2}_{\\text{erro intrínseco}}\n",
    "$$\n",
    "\n",
    "Ufa! Este é um resultado famoso de machine learning, conhecido como \"Bias/Variance tradeoff\". Representa o seguinte: para um regressor qualquer o MSE esperado é composto de três partes:\n",
    "\n",
    "- *Bias* (ou viés): O termo $E_{\\mathbf{x}} \\left[ \\left( t(\\mathbf{x}) - \\mu_{h}(\\mathbf{x}) \\right)^2 \\right]$ é uma componente do erro que independe do conjunto de testes, e que mede a competência intrínseca do modelo. Modelos muito simples (*underfitting*) tem essa componente de viés alta, pois trata-se de um erro sistêmico.\n",
    "\n",
    "- *Variance*: O termo $E_{\\mathbf{x}} \\left[ \\sigma_{h}^2(\\mathbf{x}) \\right]$ representa a volatilidade do modelo em relação ao conjunto de testes usado. Modelos muito complexos (*overfitting*) tem essa componente do erro alta.\n",
    "\n",
    "- Erro intrínseco: Esta componente do erro está relacionada com a qualidade do dado adquirido: quanto mais ruidoso, pior o desempenho ótimo do nosso sistema.\n",
    "\n",
    "O qualificador *trade-off* vem do fato de que quando o *bias* aumenta, cai a *variance*, e vice versa. Existe um ponto de ótimo, contudo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade:**\n",
    "\n",
    "Explique, em termos de *bias*, *variance* e erro intrínseco, o comportamento do exemplo do polinômio de grau 3 acima, ou seja, explique a seguinte figura:\n",
    "\n",
    "![comparação](comparacao_fits.png \"comparação entre fits\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curvas de aprendizado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O que quer dizer que um modelo é muito complexo? É um modelo que tem parâmetros de ajuste em excesso *em relação ao volume de dados de teste*. Um modelo tido como complexo para um conjunto pequeno de dados pode ser um modelo muito simples ao se considerar um conjunto maior de dados. Vamos explorar esse conceito em um experimento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def plot_learning_curves(model, X, y, ax1, ax2, step=5):\n",
    "    # Separa 20% para validação.\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X,\n",
    "        y,\n",
    "        test_size=0.2,\n",
    "        random_state=RAND_SEED,\n",
    "    )\n",
    "\n",
    "    train_errors, val_errors = [], []\n",
    "\n",
    "    # Para cada tamanho do conjunto de treinamento...\n",
    "    for m in range(1, len(X_train)):\n",
    "        mean_RMSE_train = 0.0\n",
    "        mean_RMSE_val = 0.0\n",
    "\n",
    "        count = 0\n",
    "        for i in range(0, len(X_train) - m, step):\n",
    "            # Ajusta modelo com apenas m amostras de treinamento.\n",
    "            model.fit(X_train[i:(i + m)], y_train[i:(i + m)])\n",
    "\n",
    "            # Faz a previsão sobre o conjunto de treinamento\n",
    "            # (para obter o erro de treinamento)\n",
    "            # e sobre o conjunto de validação.\n",
    "            y_train_predict = model.predict(X_train[i:(i + m)])\n",
    "            y_val_predict = model.predict(X_val)\n",
    "\n",
    "            # Calcula o valor do RMSE\n",
    "            RMSE_train = np.sqrt(\n",
    "                mean_squared_error(\n",
    "                    y_train_predict,\n",
    "                    y_train[i:(i + m)],\n",
    "                ))\n",
    "            RMSE_val = np.sqrt(mean_squared_error(y_val_predict, y_val))\n",
    "\n",
    "            mean_RMSE_train += RMSE_train\n",
    "            mean_RMSE_val += RMSE_val\n",
    "            count += 1\n",
    "\n",
    "        mean_RMSE_train /= count\n",
    "        mean_RMSE_val /= count\n",
    "\n",
    "        # Armazena ambos os valores de RMSE.\n",
    "        train_errors.append(mean_RMSE_train)\n",
    "        val_errors.append(mean_RMSE_val)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(np.sqrt(train_errors), \"r-\", linewidth=2, label=\"train\")\n",
    "    plt.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label=\"val\")\n",
    "    plt.axis(ax1)\n",
    "    plt.legend(loc=\"upper right\", fontsize=14)\n",
    "    plt.xlabel(\"Training set size\", fontsize=14)\n",
    "    plt.ylabel(\"RMSE\", fontsize=14)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(np.sqrt(train_errors), \"r-\", linewidth=2, label=\"train\")\n",
    "    plt.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label=\"val\")\n",
    "    plt.axis(ax2)\n",
    "    plt.legend(loc=\"upper right\", fontsize=14)\n",
    "    plt.xlabel(\"Training set size\", fontsize=14)\n",
    "    plt.ylabel(\"RMSE\", fontsize=14)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves(poly_reg_1, X, y, [0, 80, 0, 3], [0, 20, 0, 2], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves(poly_reg_2, X, y, [0, 80, 0, 1e1], [0, 20, 0, 2], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves(poly_reg_30, X, y, [0, 80, 0, 4e20], [0, 80, 0, 2], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_reg_10 = Pipeline([\n",
    "    (\"poly_features\", PolynomialFeatures(degree=10, include_bias=False)),\n",
    "    (\"std_scaler\", StandardScaler()),\n",
    "    (\"lin_reg\", LinearRegression()),\n",
    "])\n",
    "\n",
    "plot_learning_curves(poly_reg_10, X, y, [0, 80, 0, 500], [0, 80, 0, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar quando um ajuste passa de *overfit* para ajuste normal conforme o número de dados de treinamento aumenta!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Atividade:**\n",
    "\n",
    "Explique:\n",
    "\n",
    "- Porque o erro do modelo ``poly_reg_1`` é maior que o erro do modelo ``poly_reg_2``?\n",
    "\n",
    "- Porque o erro do modelo ``poly_reg_30`` é maior que o erro do modelo ``poly_reg_2``?\n",
    "\n",
    "- Porque na curva RMSE versus tamanho do conjunto de treinamento observamos, para o modelo de grau 10, que o erro de treinamento é zero até que o conjunto de treinamento tenha mais de 10 amostras?\n",
    "\n",
    "- Para as curvas relacionadas a modelos complexos, eventualmente o erro tende a aproximadamente $\\text{RMSE} = 1$ neste problema-exemplo. Explique em termos de *bias*, variância, e erro intrínseco. (Compare com os dados originais.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "70f49f041679d74ed38be3dd5d541915742122842326991e72b7f203061ffb6e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
